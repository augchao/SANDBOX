{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/brown' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\August Chao/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\envs\\\\new_env\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\envs\\\\new_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\August Chao\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-82016fe7aab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mbrown_tags_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;31m# sent is a list of word/tag pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# add START/START at the beginning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\new_env\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\new_env\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/brown' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\August Chao/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\envs\\\\new_env\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\envs\\\\new_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\August Chao\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# Hidden Markov Models in Python\n",
    "# Katrin Erk, March 2013 updated March 2016\n",
    "#\n",
    "# This HMM addresses the problem of part-of-speech tagging. It estimates\n",
    "# the probability of a tag sequence for a given word sequence as follows:\n",
    "#\n",
    "# Say words = w1....wN\n",
    "# and tags = t1..tN\n",
    "#\n",
    "# then\n",
    "# P(tags | words) is_proportional_to  product P(ti | t{i-1}) P(wi | ti)\n",
    "#\n",
    "# To find the best tag sequence for a given sequence of words,\n",
    "# we want to find the tag sequence that has the maximum P(tags | words)\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Estimating P(wi | ti) from corpus data using Maximum Likelihood Estimation (MLE):\n",
    "# P(wi | ti) = count(wi, ti) / count(ti)\n",
    "#\n",
    "# We add an artificial \"start\" tag at the beginning of each sentence, and\n",
    "# We add an artificial \"end\" tag at the end of each sentence.\n",
    "# So we start out with the brown tagged sentences,\n",
    "# add the two artificial tags,\n",
    "# and then make one long list of all the tag/word pairs.\n",
    "\n",
    "brown_tags_words = [ ]\n",
    "for sent in brown.tagged_sents():\n",
    "    # sent is a list of word/tag pairs\n",
    "    # add START/START at the beginning\n",
    "    brown_tags_words.append( (\"START\", \"START\") )\n",
    "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
    "    # shorten tags to 2 characters each\n",
    "    brown_tags_words.extend([ (tag[:2], word) for (word, tag) in sent ])\n",
    "    # then END/END\n",
    "    brown_tags_words.append( (\"END\", \"END\") )\n",
    "\n",
    "# conditional frequency distribution\n",
    "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
    "# conditional probability distribution\n",
    "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
    "\n",
    "print(\"The probability of an adjective (JJ) being 'new' is\", cpd_tagwords[\"JJ\"].prob(\"new\"))\n",
    "print(\"The probability of a verb (VB) being 'duck' is\", cpd_tagwords[\"VB\"].prob(\"duck\"))\n",
    "\n",
    "# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n",
    "# P(ti | t{i-1}) = count(t{i-1}, ti) / count(t{i-1})\n",
    "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
    "\n",
    "# make conditional frequency distribution:\n",
    "# count(t{i-1} ti)\n",
    "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
    "# make conditional probability distribution, using\n",
    "# maximum likelihood estimate:\n",
    "# P(ti | t{i-1})\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "\n",
    "print(\"If we have just seen 'DT', the probability of 'NN' is\", cpd_tags[\"DT\"].prob(\"NN\"))\n",
    "print( \"If we have just seen 'VB', the probability of 'JJ' is\", cpd_tags[\"VB\"].prob(\"DT\"))\n",
    "print( \"If we have just seen 'VB', the probability of 'NN' is\", cpd_tags[\"VB\"].prob(\"NN\"))\n",
    "\n",
    "\n",
    "###\n",
    "# putting things together:\n",
    "# what is the probability of the tag sequence \"PP VB TO VB\" for the word sequence \"I want to race\"?\n",
    "# It is\n",
    "# P(START) * P(PP|START) * P(I | PP) *\n",
    "#            P(VB | PP) * P(want | VB) *\n",
    "#            P(TO | VB) * P(to | TO) *\n",
    "#            P(VB | TO) * P(race | VB) *\n",
    "#            P(END | VB)\n",
    "#\n",
    "# We leave aside P(START) for now.\n",
    "\n",
    "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
    "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"want\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"TO\") * cpd_tagwords[\"TO\"].prob(\"to\") * \\\n",
    "    cpd_tags[\"TO\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"race\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"END\")\n",
    "\n",
    "print( \"The probability of the tag sequence 'START PP VB TO VB END' for 'I want to race' is:\", prob_tagsequence)\n",
    "\n",
    "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
    "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"saw\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"her\") * \\\n",
    "    cpd_tags[\"PP\"].prob(\"NN\") * cpd_tagwords[\"NN\"].prob(\"duck\") * \\\n",
    "    cpd_tags[\"NN\"].prob(\"END\")\n",
    "\n",
    "print( \"The probability of the tag sequence 'START PP VB PP NN END' for 'I saw her duck' is:\", prob_tagsequence)\n",
    "\n",
    "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
    "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"saw\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"her\") * \\\n",
    "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"duck\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"END\")\n",
    "\n",
    "print( \"The probability of the tag sequence 'START PP VB PP VB END' for 'I saw her duck' is:\", prob_tagsequence)\n",
    "\n",
    "\n",
    "#####\n",
    "# Viterbi:\n",
    "# If we have a word sequence, what is the best tag sequence?\n",
    "#\n",
    "# The method above lets us determine the probability for a single tag sequence.\n",
    "# But in order to find the best tag sequence, we need the probability\n",
    "# for _all_ tag sequence.\n",
    "# What Viterbi gives us is just a good way of computing all those many probabilities\n",
    "# as fast as possible.\n",
    "\n",
    "# what is the list of all tags?\n",
    "distinct_tags = set(brown_tags)\n",
    "\n",
    "sentence = [\"I\", \"want\", \"to\", \"race\" ]\n",
    "#sentence = [\"I\", \"saw\", \"her\", \"duck\" ]\n",
    "sentlen = len(sentence)\n",
    "\n",
    "# viterbi:\n",
    "# for each step i in 1 .. sentlen,\n",
    "# store a dictionary\n",
    "# that maps each tag X\n",
    "# to the probability of the best tag sequence of length i that ends in X\n",
    "viterbi = [ ]\n",
    "\n",
    "# backpointer:\n",
    "# for each step i in 1..sentlen,\n",
    "# store a dictionary\n",
    "# that maps each tag X\n",
    "# to the previous tag in the best tag sequence of length i that ends in X\n",
    "backpointer = [ ]\n",
    "\n",
    "first_viterbi = { }\n",
    "first_backpointer = { }\n",
    "for tag in distinct_tags:\n",
    "    # don't record anything for the START tag\n",
    "    if tag == \"START\": continue\n",
    "    first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[0] )\n",
    "    first_backpointer[ tag ] = \"START\"\n",
    "\n",
    "print(first_viterbi)\n",
    "print(first_backpointer)\n",
    "    \n",
    "viterbi.append(first_viterbi)\n",
    "backpointer.append(first_backpointer)\n",
    "\n",
    "currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
    "print( \"Word\", \"'\" + sentence[0] + \"'\", \"current best two-tag sequence:\", first_backpointer[ currbest], currbest)\n",
    "# print( \"Word\", \"'\" + sentence[0] + \"'\", \"current best tag:\", currbest)\n",
    "\n",
    "for wordindex in range(1, len(sentence)):\n",
    "    this_viterbi = { }\n",
    "    this_backpointer = { }\n",
    "    prev_viterbi = viterbi[-1]\n",
    "    \n",
    "    for tag in distinct_tags:\n",
    "        # don't record anything for the START tag\n",
    "        if tag == \"START\": continue\n",
    "\n",
    "        # if this tag is X and the current word is w, then \n",
    "        # find the previous tag Y such that\n",
    "        # the best tag sequence that ends in X\n",
    "        # actually ends in Y X\n",
    "        # that is, the Y that maximizes\n",
    "        # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
    "        # The following command has the same notation\n",
    "        # that you saw in the sorted() command.\n",
    "        best_previous = max(prev_viterbi.keys(),\n",
    "                            key = lambda prevtag: \\\n",
    "            prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
    "\n",
    "        # Instead, we can also use the following longer code:\n",
    "        # best_previous = None\n",
    "        # best_prob = 0.0\n",
    "        # for prevtag in distinct_tags:\n",
    "        #    prob = prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex])\n",
    "        #    if prob > best_prob:\n",
    "        #        best_previous= prevtag\n",
    "        #        best_prob = prob\n",
    "        #\n",
    "        this_viterbi[ tag ] = prev_viterbi[ best_previous] * \\\n",
    "            cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[wordindex])\n",
    "        this_backpointer[ tag ] = best_previous\n",
    "\n",
    "    currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
    "    print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best two-tag sequence:\", this_backpointer[ currbest], currbest)\n",
    "    # print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best tag:\", currbest)\n",
    "\n",
    "\n",
    "    # done with all tags in this iteration\n",
    "    # so store the current viterbi step\n",
    "    viterbi.append(this_viterbi)\n",
    "    backpointer.append(this_backpointer)\n",
    "\n",
    "\n",
    "# done with all words in the sentence.\n",
    "# now find the probability of each tag\n",
    "# to have \"END\" as the next tag,\n",
    "# and use that to find the overall best sequence\n",
    "prev_viterbi = viterbi[-1]\n",
    "best_previous = max(prev_viterbi.keys(),\n",
    "                    key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
    "\n",
    "prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
    "\n",
    "# best tagsequence: we store this in reverse for now, will invert later\n",
    "best_tagsequence = [ \"END\", best_previous ]\n",
    "# invert the list of backpointers\n",
    "backpointer.reverse()\n",
    "\n",
    "# go backwards through the list of backpointers\n",
    "# (or in this case forward, because we have inverter the backpointer list)\n",
    "# in each case:\n",
    "# the following best tag is the one listed under\n",
    "# the backpointer for the current best tag\n",
    "current_best_tag = best_previous\n",
    "for bp in backpointer:\n",
    "    best_tagsequence.append(bp[current_best_tag])\n",
    "    current_best_tag = bp[current_best_tag]\n",
    "\n",
    "best_tagsequence.reverse()\n",
    "print \"The sentence was:\",\n",
    "for w in sentence: print w\n",
    "print \"\\n\"  \n",
    "print \"The best tag sequence is:\", \n",
    "for t in best_tagsequence: print t,\n",
    "print \"\\n\"\n",
    "print \"The probability of the best tag sequence is:\", prob_tagsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
